# Crawler#

Crawler реализуется в виде консольной утилиты с тремя командами: update, download, clean.

a) Команда update позволяет найти и сохранить ссылки на еще не скачанные статьи, опубликованные в определенный промежуток времени. Стоит заметить, что логика данной команды основана на переборе всех ссылок и выделении тех из них, которые были опубликованы в заданный промежуток времени (ссылка на статью имеет вид "http://www.engadget.com/2016/02/26/mit-wave-warning-system/", дата публикации извлекается достаточно просто с помощью регулярных выражений). 

У update следующие ключи:

        --help - Для получения инструкции к команде

        --from - Задает начальную дату публикации искомых статей

        --to - Задает конечную дату публикации статей

        --print - Печатает найденные ссылки

b) Команда download по полученному в результате работы команды update списку ссылок или по списку ссылок, заданному как параметр команды, скачивает html-код страниц, находящихся по данным ссылкам. Страницы сохраняются в формате html, имя файла - название статьи (ссылка на статью имеет вид "http://www.engadget.com/2016/02/26/mit-wave-warning-system/", из нее достаточно просто достать название статьи).

Команда имеет следующие ключи:

        --help - инструкция

        --all - скачивание всех статей из списка статей, полученных update

        [urls] - скачивание только определенных статей

        -t - количество потоков

c) Команда clean отчищает html-код статьи от "мусора", оставляя только название и текст. Она будет открывать файл с html кодом странички, используя HTMLParser из библиотеки html искать там теги, в которых содержатся заголовок и текст статьи, и сохранять их содержимое в новый txt файл. Также clean должна обновлять информацию о количестве отчищенных файлов в файле info.

У clean реализованы следующие ключи:

        --help - инструкция

        --all - отчистка всех загруженных, но не отчищенных статей

        [urls] - отчистка определенной статьи


# Оценка статей #

Консольная утилита mark позволяет ставить оценки новым, еще не оцененным статьям.

# Классификация статей #

ML_notebook - ipython3 Notebook,с реализованным линейным классификатором на основе модели мешка слов, также применяется процедура кросс-валидации.

#Используемые технологии#

Библитотеки sklearn, nltk, re, urllib, html, io, argparse, sys, os, collections, traceback, logging, random, datetime.